import os
import shutil
import glob
import google.generativeai as genai
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from dotenv import load_dotenv

# Env yÃ¼kle
load_dotenv()

# Google Gemini API KonfigÃ¼rasyonu
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    print("âŒ HATA: GOOGLE_API_KEY .env dosyasÄ±nda bulunamadÄ±!")
    
genai.configure(api_key=GOOGLE_API_KEY)

class RAGService:
    def __init__(self):
        self.vector_store = None
        # PDF tarama iÃ§in TÃ¼rkÃ§e destekli gÃ¼Ã§lÃ¼ bir model
        self.embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
        self.persist_directory = "./vector_store"
        
        # Gemini Modeli BaÅŸlatma
        try:
            self.model = genai.GenerativeModel("gemini-2.5-flash")
            print(">>> [Gemini] Model baÅŸarÄ±yla baÅŸlatÄ±ldÄ±: gemini-2.5-flash")
        except Exception as e:
            print(f"!!! [Gemini] Model baÅŸlatma hatasÄ±: {e}")
            self.model = None

    def ingest_documents(self):
        """Knowledge Base klasÃ¶rÃ¼ndeki PDF'leri okur ve vektÃ¶r yapar."""
        
        if not os.path.exists("./knowledge_base"):
            os.makedirs("./knowledge_base")
            print(">>> [UYARI] 'knowledge_base' klasÃ¶rÃ¼ oluÅŸturuldu. LÃ¼tfen iÃ§ine PDF ekleyin.")
            return

        # Mevcut PDF sayÄ±sÄ±nÄ± kontrol et
        pdf_files = glob.glob("./knowledge_base/*.pdf")
        current_file_count = len(pdf_files)
        
        # KayÄ±tlÄ± dosya sayÄ±sÄ±nÄ± kontrol et
        count_file_path = os.path.join(self.persist_directory, "file_count.txt")
        saved_file_count = -1
        
        if os.path.exists(count_file_path):
            with open(count_file_path, "r") as f:
                try:
                    saved_file_count = int(f.read().strip())
                except:
                    pass

        # --- HIZLI BAÅLANGIÃ‡ KONTROLÃœ ---
        # EÄŸer dosya sayÄ±sÄ± aynÄ±ysa ve DB varsa tekrar yÃ¼kleme
        index_file = os.path.join(self.persist_directory, "chroma.sqlite3")
        if os.path.exists(self.persist_directory) and os.path.exists(index_file) and current_file_count == saved_file_count:
            print(f">>> [RAG] Mevcut veritabanÄ± gÃ¼ncel ({current_file_count} dosya), yÃ¼kleniyor...")
            self.vector_store = Chroma(
                persist_directory=self.persist_directory, 
                embedding_function=self.embedding_model
            )
            return
        # --------------------------------

        print(f"--- ğŸ”„ DeÄŸiÅŸiklik AlgÄ±landÄ± veya Ä°lk Kurulum (PDF SayÄ±sÄ±: {current_file_count}) ---")
        print("--- â³ PDF'ler Yeniden Ä°ndeksleniyor (LÃ¼tfen bekleyin)... ---")
        
        # Eski veritabanÄ±nÄ± temizle
        if os.path.exists(self.persist_directory):
            try:
                shutil.rmtree(self.persist_directory)
                print(">>> [RAG] Eski veritabanÄ± temizlendi.")
            except Exception as e:
                print(f"!!! [UYARI] Eski veritabanÄ± silinemedi: {e}")

        try:
            loader = PyPDFDirectoryLoader("./knowledge_base")
            docs = loader.load()
            
            if not docs:
                print(">>> [UYARI] KlasÃ¶rde PDF bulunamadÄ±.")
                return

            splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000, 
                chunk_overlap=200,
                separators=["\n\n", "\n", " ", ""]
            )
            chunks = splitter.split_documents(docs)
            
            self.vector_store = Chroma.from_documents(
                documents=chunks, 
                embedding=self.embedding_model,
                persist_directory=self.persist_directory
            )
            
            # Dosya sayÄ±sÄ±nÄ± kaydet
            with open(count_file_path, "w") as f:
                f.write(str(current_file_count))
                
            print(f"--- âœ… {len(chunks)} ParÃ§a Veri BaÅŸarÄ±yla Ä°ndekslendi ve Kaydedildi ---")
        except Exception as e:
            print(f"!!! Ä°ndeksleme HatasÄ±: {e}")

    def get_relevant_context(self, query):
        """Soruyu vektÃ¶re Ã§evirip veritabanÄ±nda en alakalÄ± kÄ±sÄ±mlarÄ± arar."""
        if not self.vector_store:
            # EÄŸer bellekte yoksa diskten yÃ¼klemeyi dene
            if os.path.exists(self.persist_directory):
                self.vector_store = Chroma(persist_directory=self.persist_directory, embedding_function=self.embedding_model)
            else:
                return ""
            
        try:
            docs = self.vector_store.similarity_search(query, k=3)
            # EÄŸer belge bulunamazsa boÅŸ dÃ¶n
            if not docs:
                return ""
            context_text = "\n\n".join([doc.page_content for doc in docs])
            return context_text
        except Exception as e:
            print(f"Context alma hatasÄ±: {e}")
            return ""

    def ask_kaggle_llm(self, context, question):
        """
        ArtÄ±k Kaggle yerine doÄŸrudan Google Gemini API (Flash) kullanÄ±yor.
        Fonksiyon ismi geriye uyumluluk iÃ§in deÄŸiÅŸtirilmedi.
        """
        if not self.model:
            return "Hata: Gemini modeli baÅŸlatÄ±lamadÄ±. API Key kontrol edin."

        is_context_empty = False
        if not context or not context.strip():
            print("â„¹ï¸ Bilgi: Context (BaÄŸlam) boÅŸ. Model kendi genel bilgisini kullanacak.")
            is_context_empty = True
            context = "Bu soru iÃ§in veritabanÄ±nda Ã¶zel bir dÃ¶kÃ¼man bulunamadÄ±."

        print(f"DEBUG: Retrieved Context Snippet:\n{context[:500]}...\n")
        
        full_prompt = (
            "Sen yardÄ±msever bir tÄ±bbi yapay zeka asistanÄ±sÄ±n.\n"
            "GÃ¶revin, kullanÄ±cÄ±nÄ±n tÄ±bbi sorularÄ±nÄ± yanÄ±tlamaktÄ±r.\n\n"
            "YÃ–NERGELER:\n"
            "1. AÅŸaÄŸÄ±da 'BaÄŸlam Bilgisi' (Context) verilecektir. Ã–ncelikle bu bilgiyi kontrol et.\n"
            "2. EÄŸer sorunun cevabÄ± baÄŸlam iÃ§inde varsa, bu kaynaÄŸÄ± kullanarak cevap ver.\n"
            "3. EÄŸer baÄŸlam boÅŸsa veya sorunun cevabÄ±nÄ± iÃ§ermiyorsa, **kendi genel tÄ±bbi bilgini kullanarak** en doÄŸru ve gÃ¼venilir cevabÄ± ver.\n"
            "4. **Ã–NEMLÄ°:** Cevaba doÄŸrudan baÅŸla. 'BaÄŸlamda bilgi yok, bu yÃ¼zden genel bilgimi kullanÄ±yorum' gibi giriÅŸ cÃ¼mleleri **KESÄ°NLÄ°KLE KURMA**.\n"
            "5. CevabÄ±n aÃ§Ä±klayÄ±cÄ±, yardÄ±msever ve TÃ¼rkÃ§e olsun.\n\n"
            f"BaÄŸlam Bilgisi (Context):\n{context}\n\n"
            f"KullanÄ±cÄ± Sorusu: {question}\n"
        )
        
        print(f"ğŸ“¡ Google Gemini API'ye baÄŸlanÄ±lÄ±yor...")
        
        try:
            # Gemini generation config
            config = genai.GenerationConfig(
                temperature=0.3, # Biraz daha esneklik iÃ§in 0.3
                max_output_tokens=2048, # YanÄ±tÄ±n kesilmemesi iÃ§in artÄ±rÄ±ldÄ±
            )
            
            # Safety Settings: TÄ±bbi sorularÄ±n engellenmemesi iÃ§in
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_ONLY_HIGH"
                },
            ]
            
            response = self.model.generate_content(
                full_prompt,
                generation_config=config,
                safety_settings=safety_settings
            )
            
            print("âœ… Gemini Cevap Verdi.")
            return response.text
                
        except Exception as e:
            print(f"âŒ Gemini API HatasÄ±: {e}")
            return f"Model HatasÄ±: {str(e)}"